**Insurance Premium Prediction - Kaggle Competition**

ðŸš€ **Machine Learning | LightGBM | Feature Engineering | Hyperparameter Tuning**

**Project Overview**

In this Kaggle competition, I developed a LightGBM regression model to predict insurance premiums, **achieving an RMSLE score of 1.07**. The dataset consisted of **1.4M** training and **800K** test observations, requiring extensive EDA, missing value handling (**3% to 30%**), and feature engineeringâ€”with **four features ranking in the top 10** for model importance.

ðŸ”¹ **Key Techniques Used:**
â€¢	**Exploratory Data Analysis (EDA):** Identified data distribution, outliers, and feature relationships.

â€¢	**Handling Missing Data:** Applied group-wise median imputation for efficiency.

â€¢	**Feature Engineering & Selection:** Created impactful features, boosting model performance.

â€¢	**Target Variable Transformation:** Applied log transformation to stabilize variance.

â€¢	**Hyperparameter Tuning:** Used RandomizedSearchCV for efficient optimization.

â€¢	**Stratified Sampling for Model Training:** Ensured balanced representation of premium amounts.

ðŸŽ¯ **Final Results:** 

â€¢	Initial RMSLE Score: **1.15912**

â€¢	Optimized RMSLE Score: **1.07849**

ðŸ”¹ **Dataset Details**
â€¢	**Train Set:** **1.4M** observations

â€¢	**Test Set:** **800K** observations

â€¢	**Target Variable:** **premium_amount**

â€¢ **Metric:**  **RMSLE**

**Approach & Methodology**   
âœ” **Data Preprocessing:** Cleaned missing values and optimized categorical encoding.

âœ” **Feature Engineering:** Created new variables that enhanced model interpretability.

âœ” **Model Selection:** Implemented LightGBM, leveraging its efficiency with large datasets.

âœ” **Hyperparameter Tuning:** Used RandomizedSearchCV to optimize performance.

